plot(ridge.mod, xvar="lambda", label=T)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
#Mean Prediction Error
ridge.pred=predict(ridge.mod ,s=bestlam ,newx=x.test)
mean((ridge.pred -y.test)^2)
sd((ridge.pred -y.test)^2)/sqrt(length((ridge.pred -y.test)^2))
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
ridge.coef
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x.test)
mean((lasso.pred -y.test)^2)
sd((lasso.pred -y.test)^2)/sqrt(length((lasso.pred -y.test)^2))
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
lasso.coef
library(lars)
library(GGally)
library(ggplot2)
library (gridExtra)
library(corrgram)
library(corrplot)
library(leaps)
library(glmnet)
data(diabetes)
data.all <- data.frame(cbind(diabetes$x, y = diabetes$y))
dim(data.all)
names(data.all)
summary(data.all)
str(data.all)
n <- dim(data.all)[1] # sample size = 442
set.seed(1306) # set random number generator seed to enable
test <- sample(n, round(n/4)) # randomly sample 25% test
data.train <- data.all[-test,]
data.test <- data.all[test,]
x <- model.matrix(y ~ ., data = data.all)[,-1] # define predictor matrix
##Create matrix with only response variable
y <- data.all$y # define response variable
# Create training matrix
x.train <- x[-test,] # define training predictor matrix
x.test <- x[test,] # define test predictor matrix
##Create individual y matrix
y.train <- y[-test] # define training response variable
y.test <- y[test] # define test response variable
#Store number of obversations for each training and test set
n.train <- dim(data.train)[1] # training sample size = 332
n.test <- dim(data.test)[1] # test sample size = 110
model.lm <- lm(y~.,data = data.train)
lm.summary <- summary(model.lm)
lm.summary
lm.pred <- predict(model.lm,data.test)
mean((lm.pred -data.test$y)^2)
sd((lm.pred -data.test$y)^2)/sqrt(length((lm.pred -data.test$y)^2))
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form ,newdata )
coefi =coef(object ,id=id)
xvars =names(coefi )
mat[,xvars ]%*% coefi
}
regfit.full=regsubsets(y~.,data.train,nvmax =10)
reg.summary <- summary(regfit.full)
which.min(reg.summary$bic)
#Explore graph
plot(reg.summary$bic ,xlab=" Number of Variables ",ylab="BIC",
type="l")
points(6, reg.summary$bic[6], col =" red",cex =2, pch =20)
#Get variables and coefficients
coef(regfit.full ,6)
regfit.pred <- predict.regsubsets(regfit.full,data.test,id=5)
mean((regfit.pred -data.test$y)^2)
sd((regfit.pred -data.test$y)^2)/sqrt(length((regfit.pred -data.test$y)^2))
k=10
set.seed(1306)
folds <- sample(1:k, nrow(data.train), replace = TRUE)
cv.errors <- matrix(NA ,k,10, dimnames =list(NULL , paste (1:10) ))
for(j in 1:k){
best.fit=regsubsets(y~.,data=data.train[folds !=j,],
nvmax =10)
for(i in 1:10) {
pred=predict(best.fit ,data.train[folds ==j,], id=i)
cv.errors[j,i]=mean( (data.train$y[folds ==j]-pred)^2)
}
}
mean.cv.errors =apply(cv.errors ,2, mean)
min(mean.cv.errors)
# [1] 2978.907
par(mfrow =c(1,1))
plot(mean.cv.errors ,type='b')
reg.best=regsubsets (y~.,data=data.train , nvmax =10)
coef(reg.best ,6)
#Coefficients:
regbest.pred <- predict.regsubsets(reg.best,data.test,id=6)
mean((regbest.pred -data.test$y)^2)
sd((regbest.pred -data.test$y)^2)/sqrt(length((regbest.pred -data.test$y)^2))
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
ridge.pred=predict(ridge.mod ,s=bestlam ,newx=x.test)
mean((ridge.pred -y.test)^2)
sd((ridge.pred -y.test)^2)/sqrt(length((ridge.pred -y.test)^2))
ridge.coef
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
ridge.coef
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
# [1] 4.791278
#Mean Prediction Error:
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x.test)
mean((lasso.pred -y.test)^2)
# [1] 2920.08
#SE of MSE
sd((lasso.pred -y.test)^2)/sqrt(length((lasso.pred -y.test)^2))
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
lasso.coef
save.image("~/Documents/Work_Computer/Grad_School/PREDICT_422/Module4/IndividualProject1/Project1.RData")
p1<-ggplot(aes(x=age),
data = data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Age')
p2<-ggplot(aes(x=sex),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Gender')
p3<-ggplot(aes(x=bmi),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('BMI')
p4<-ggplot(aes(x=map),data=data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Avg Blood Pressure')
p5<-ggplot(aes(x=tc),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('tc')
p6<-ggplot(aes(x=ldl),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('LDL')
p7<-ggplot(aes(x=hdl),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('HDL')
p8<-ggplot(aes(x=tch),data=data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('TCH')
p9<-ggplot(aes(x=ltg),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('LTG')
p10<-ggplot(aes(x=glu),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Glucose')
p11<-ggplot(aes(x=y),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Quantitative Progression of Disease')
grid.arrange(p1,p2,p3,p3,p4,p5,p6,p7,p8,p9,p10,p11,ncol=3)
#There is no missing data, all variables are numeric, there are 442 data points and 11 variables
p11<-ggplot(aes(x=y),           data =  data.all ) +  geom_histogram(color =I('black'),fill = I('#099009'))+  ggtitle('Disease Progression')
p11<-ggplot(aes(x=y),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Disease Progression')
grid.arrange(p1,p2,p3,p3,p4,p5,p6,p7,p8,p9,p10,p11,ncol=3)
dim(data.all)
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,ncol=3)
coef(regfit.full ,6)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
plot(cv.out)
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
plot(cv.out)
log(bestlam)
plot(lasso.mod, xvar="lambda", label=T)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
log(bestlam)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
plot(cv.out)
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
ridge.coef
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:11,]
ridge.coef
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:11 ,]
lasso.coef
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
# [1] 4.791278
#Mean Prediction Error:
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x.test)
mean((lasso.pred -y.test)^2)
# [1] 2920.08
#SE of MSE
sd((lasso.pred -y.test)^2)/sqrt(length((lasso.pred -y.test)^2))
# [1] 346.228
##Coefficients:
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:11 ,]
lasso.coef
regfit.pred <- predict.regsubsets(regfit.full,data.test,id=6)
mean((regfit.pred -data.test$y)^2)
sd((regfit.pred -data.test$y)^2)/sqrt(length((regfit.pred -data.test$y)^2))
save.image("~/Documents/Work_Computer/Grad_School/PREDICT_422/Module4/IndividualProject1/Project1.RData")
oib_isp <- summaryBy(inbox_rate ~ COMMON_NAME,data=oib,FUN= mean,na.rm=TRUE)
library(doBy)
oib_isp <- summaryBy(inbox_rate ~ COMMON_NAME,data=oib,FUN= mean,na.rm=TRUE)
load("~/Documents/Work_Computer/Grad_School/PREDICT_422/PREDICT422_GroupProject/individual_code/GroupProject_sheetsa.RData")
summary.gbm
summary.gbm(model.boost1)
library(gbm)
summary.gbm(model.boost1)
setwd("/Users/asheets/Documents/Work_Computer/Grad_School/PREDICT_422/PREDICT422_GroupECProject")
data <- read.csv(file='Speed_Dating_Data.csv',header=TRUE,sep=",",stringsAsFactors=FALSE)
data <- read.csv(file='./GroupProjectWork/Speed_Dating_Data.csv',header=TRUE,sep=",",stringsAsFactors=FALSE)
data <- read.csv(file='./GroupProjectWork/Kaggle_Resources/Speed_Dating_Data.csv',header=TRUE,sep=",",stringsAsFactors=FALSE)
data <- read.csv(file='./GroupProjectWork/Kaggle_Resources/Speed_Dating_Data.csv',header=TRUE,sep=",",stringsAsFactors=FALSE)
#Install Packages if they don't current exist on this machine
list.of.packages <- c("doBy"
,"lazyeval"
,"psych"
,"lars"
,"GGally"
,"ggplot2"
,"gridExtra"
,"corrgram"
,"corrplot"
,"leaps"
,"glmnet"
,"MASS"
,"gbm"
,"tree"
,"rpart"
,"rpart.plot"
,"gam"
,"class"
,"e1071"
,"randomForest"
,"doParallel"
,"iterators"
,"foreach"
,"parallel")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only = TRUE)
table(complete.cases(data))
data2 <- data[which(data$wave %in% c(1:5,10:21)),]
data2 <- data2[,c(3,7:8,10,13:17,34,40:42,46:48,51:68,70:75,82:92)]
data2 <- data2[complete.cases(data2),]
dim(data2)
plots <- vector("list", 51)
names <- colnames(data)
plot_vars = function (data2, column)
ggplot(data = data, aes_string(x = column)) +
geom_histogram(color =I('black'),fill = I('#099009'))+
xlab(column)
plots <- lapply(colnames(data2)[1:51], plot_vars, data = data2)
n <- length(plots)
nCol <- floor(sqrt(n))
do.call("grid.arrange", c(plots))
nums <- data.frame(sapply(data2, is.numeric))
char <- data.frame(sapply(data2, is.character))
nums$variable <- rownames(nums)
char$variable <- rownames(char)
nums <- rownames(nums[which(nums$sapply.data2..is.numeric. == TRUE),])
char <- rownames(char[which(char$sapply.data2..is.character. == TRUE),])
correlations <- data.frame(cor(data2[,c(nums)], use="complete.obs", method="pearson") )
M <- cor(data2[,c(nums)], use="complete.obs", method="pearson")
#
significant.correlations <- data.frame(
var1 = character(),
var2 = character(),
corr = numeric())
#
for (i in 1:nrow(correlations)){
for (j in 1:ncol(correlations)){
tmp <- data.frame(
var1 = as.character(colnames(correlations)[j]),
var2 = as.character(rownames(correlations)[i]),
corr = correlations[i,j])
#
if (!is.na(correlations[i,j])) {
if (correlations[i,j] > 0.5 & as.character(tmp$var1) != as.character(tmp$var2)
| correlations[i,j] < -0.5 & as.character(tmp$var1) != as.character(tmp$var2) ) {
significant.correlations <- rbind(significant.correlations,tmp) }
}
}
}
significant.correlations <- significant.correlations[order(abs(significant.correlations$corr),decreasing=TRUE),]
significant.correlations <- significant.correlations[which(!duplicated(significant.correlations$corr)),]
significant.correlations
smp_size <- floor(0.75 * nrow(data2))
set.seed(22)
train_ind <- sample(seq_len(nrow(data2)), size = smp_size)
train <- data2[train_ind, ]
test <- data2[-train_ind, ]
y.train <- data.frame(match = train$match)
y.test <- data.frame(match = test$match)
x.train <- train[,c(-5)]
x.test <- test[,c(-5)]
train.mean <- apply(x.train, 2, mean)
train.sd <- apply(x.train, 2, sd)
train.std <- t((t(x.train)-train.mean)/train.sd) # standardize to have zero mean and unit sd
train.std <- cbind(y.train,train.std)
test.mean <- apply(x.test, 2, mean)
test.sd <- apply(x.test, 2, sd)
test.std <- t((t(x.test)-test.mean)/test.sd) # standardize to have zero mean and unit sd
test.std <- cbind(y.test,test.std)
set.seed(1)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
glm.fit=glm(match~.,
data=train.std,family = binomial)
# AIC: 4330.9
step.glm <- step(glm.fit)
summary(step.glm)
vif(step.glm)
??vif
library(var)
library(car)
??vif
vif(step.glm)
glm.final=glm(match~gender+order+int_corr+samerace+age_o+age+imprace+date+go_out+dining+museums+art+clubbing+reading+movies+concerts+
attr1_1 + sinc1_1 + shar1_1 + attr3_1 + amb3_1,
data=train.std,family = binomial)
summary(glm.final)
glm.probs <- predict(glm.final,test.std,type="response")
my_roc <- function(predicted.probs=glm.probs,actual=test.std$match) {
threshold_sensitivity <- function(predicted.probs,actual,threshold) {
confusion <- table(factor(predicted.probs >= threshold,levels=c("FALSE","TRUE")),factor(actual))
x <- (confusion[4]) / (confusion[3] + confusion[4])
return(x)
}
threshold_specificity <- function(predicted.probs,actual,threshold) {
confusion <- table(factor(predicted.probs >= threshold,levels=c("FALSE","TRUE")),factor(actual))
x <- (confusion[1]) / (confusion[1] + confusion[2])
return(x)
}
d <- data.frame(threshold = seq(0,1,by=0.01), sensitivity=NA, specificity=NA)
d$sensitivity <- sapply(d$threshold, function(x) threshold_sensitivity(predicted.probs,actual,x))
d$specificity <- sapply(d$threshold, function(x) threshold_specificity(predicted.probs,actual,x))
d$spec1 <- 1- d$specificity
p <- ggplot(d, aes(spec1,sensitivity)) +
geom_line(color="blue") +
coord_fixed() +
geom_line(aes(threshold,threshold)) +
labs(title = sprintf("ROC")) + xlab("1-Specificity") + ylab("Sensitivity")
height <- (d$sensitivity[-1]+d$sensitivity[-length(d$sensitivity)])/2
width <- -diff(d$spec1,na.rm=TRUE)
auc <- sum(height*width,na.rm=TRUE)
return(list(p,auc))
}
my_roc_stuff <- my_roc(data$scored.probability,data$class)
my_roc_stuff
table(data$class)
names(data)
names(glm.probs)
head(glm.probs)
my_roc_stuff <- my_roc(glm.probs,train.std$match)
my_roc_stuff <- my_roc(glm.probs,test.std$match)
my_roc_stuff
glm.pred <- ifelse(glm.probs >= 0.35,1,0)
confusionMatrix(glm.pred,test.std$match,positive='1')
??confusionMatrix
library(caret)
confusionMatrix(glm.pred,test.std$match,positive='1')
set.seed(1)
knn.pred=knn(train.std,test.std,train.std$match,k=7)
table(knn.pred ,test.std$match)
confusionMatrix(knn.pred,test.std$match,positive='1')
set.seed(1)
model.tree1 <- rpart(as.factor(match) ~ gender+order+int_corr+samerace+age_o+age+imprace+date+go_out+dining+museums+art+clubbing+reading+movies+concerts+
attr1_1 + sinc1_1 + shar1_1 + attr3_1 + amb3_1,data=train.std,method="class")
prp(model.tree1)
tree.pred<- predict(model.tree1,test.std)
tree.pred
set.seed(1)
model.RF1 <- randomForest(as.factor(match)~.,data=train.std,
mtry=13, ntree =501)
getTree(model.RF1, 1, labelVar=TRUE)
getConds<-function(tree){
#store all conditions into a list
conds<-list()
#start by the terminal nodes and find previous conditions
id.leafs<-which(tree$status==-1)
j<-0
for(i in id.leafs){
j<-j+1
prevConds<-prevCond(tree,i)
conds[[j]]<-prevConds$cond
while(prevConds$id>1){
prevConds<-prevCond(tree,prevConds$id)
conds[[j]]<-paste(conds[[j]]," & ",prevConds$cond)
if(prevConds$id==1){
conds[[j]]<-paste(conds[[j]]," => ",tree$prediction[i])
break()
}
}
}
return(conds)
}
getConds(model.RF1)
library(reptree)
install.packages(reptree)
install.packages(reprtree)
options(repos='http://cran.rstudio.org')
have.packages <- installed.packages()
cran.packages <- c('devtools','plotrix','randomForest','tree')
to.install <- setdiff(cran.packages, have.packages[,1])
if(length(to.install)>0) install.packages(to.install)
library(devtools)
if(!('reprtree' %in% installed.packages())){
install_github('araastat/reprtree')
}
for(p in c(cran.packages, 'reprtree')) eval(substitute(library(pkg), list(pkg=p)))
library(reprtree)
reprtree:::plot.getTree(model.RF1)
confusionMatrix(pred.RF1,test.std$match,positive='1')
pred.RF1 = predict(model.RF1,newdata = test.std)
confusionMatrix(pred.RF1,test.std$match,positive='1')
library(e1071)
set.seed(1)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
svm.tune=tune(svm,match~.,data=train.std ,kernel ="radial",ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(svm.tune)
mtry.value <- floor(sqrt(ncol(train.std)-1))
mtry.value
sqrt(50)
bestmtry <- tuneRF(train.std, as.factor(match), stepFactor=1.5, improve=1e-5, ntree=500)
names(train.std)
bestmtry <- tuneRF(train.std[,c(-1)], as.factor(match), stepFactor=1.5, improve=1e-5, ntree=500)
train.std[,c(-1)]
as.factor(match)
bestmtry <- tuneRF(train.std[,c(-1)], as.factor(train.std$match), stepFactor=1.5, improve=1e-5, ntree=500)
model.RF1 <- randomForest(as.factor(match)~.,data=train.std,
mtry=4, ntree =501)
varImpPlot(model.RF1)
importance(model.RF1)
pred.RF1 = predict(model.RF1,newdata = test.std)
confusionMatrix(pred.RF1,test.std$match,positive='1')
model.RF1 <- randomForest(as.factor(match)~.,data=train.std,
mtry=7, ntree =501)
varImpPlot(model.RF1)
pred.RF1 = predict(model.RF1,newdata = test.std)
confusionMatrix(pred.RF1,test.std$match,positive='1')
model.RF1 <- randomForest(as.factor(match)~.,data=train.std,
mtry=13, ntree =501)
pred.RF1 = predict(model.RF1,newdata = test.std)
confusionMatrix(pred.RF1,test.std$match,positive='1')
mtry.value <- floor(sqrt(ncol(train.std)-1))
model.RF2 <- randomForest(as.factor(match)~.,data=train.std,
mtry=mtry.value, ntree =501)
pred.RF2 = predict(model.RF2,newdata = test.std)
confusionMatrix(pred.RF2,test.std$match,positive='1')
model.RF3 <- randomForest(as.factor(match)~.,data=train.std,
mtry=4, ntree =501)
pred.RF3 = predict(model.RF3,newdata = test.std)
confusionMatrix(pred.RF3,test.std$match,positive='1')
varImpPlot(model.RF3)
importance(model.RF3)
?tuneRF
my_roc_stuff
my_roc_stuff <- my_roc(knn.pref,test.std$match)
my_roc_stuff <- my_roc(knn.pred,test.std$match)
knn.pred
my_roc_stuff <- my_roc(pred.svm,test.std$match)
